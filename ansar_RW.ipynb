{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc1ee9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8033ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply log transformation to the COSMIC features\n",
    "def transform_cosmic_features(df):\n",
    "    # Apply log1p (log(1 + x)) to avoid issues with log(0)\n",
    "    df.loc[:, 'COSMIC Read'] = np.log1p(df['COSMIC Read'].clip(lower=0))\n",
    "    df.loc[:, 'COSMIC Write'] = np.log1p(df['COSMIC Write'].clip(lower=0))\n",
    "    df.loc[:, 'COSMIC Entry'] = np.log1p(df['COSMIC Entry'].clip(lower=0))\n",
    "    df.loc[:, 'COSMIC Exit'] = np.log1p(df['COSMIC Exit'].clip(lower=0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "ISBG = pd.read_excel(\"./data/ISBSG-whole.xlsx\", header=3)\n",
    "\n",
    "# Select the required columns\n",
    "cols_needed = ['Max Team Size', 'COSMIC Read', 'COSMIC Write', 'COSMIC Entry', 'COSMIC Exit', \n",
    "               'Functional Size', 'Project Elapsed Time', 'Development Platform', 'Primary Programming Language', \n",
    "               'Summary Work Effort']\n",
    "ISBG_interest = ISBG[cols_needed]\n",
    "\n",
    "# Drop rows where essential COSMIC features have missing values\n",
    "df_clean = ISBG_interest.dropna(subset=[\"COSMIC Read\", \"COSMIC Write\", \"COSMIC Exit\", \"COSMIC Entry\"])\n",
    "\n",
    "# Separate features and target\n",
    "cosmic_cols = [\"COSMIC Read\", \"COSMIC Write\", \"COSMIC Entry\", \"COSMIC Exit\"]\n",
    "tech_cols = ['Max Team Size', 'Functional Size', 'Project Elapsed Time', 'Development Platform', 'Primary Programming Language']\n",
    "\n",
    "X = df_clean[cosmic_cols + tech_cols]\n",
    "y = df_clean[\"Summary Work Effort\"]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the COSMIC features before the split\n",
    "X_train_cosmic = transform_cosmic_features(X_train[cosmic_cols])\n",
    "X_test_cosmic = transform_cosmic_features(X_test[cosmic_cols])\n",
    "\n",
    "# Apply StandardScaler to the transformed COSMIC features\n",
    "scaler = StandardScaler()\n",
    "X_train_cosmic = scaler.fit_transform(X_train_cosmic)\n",
    "X_test_cosmic = scaler.transform(X_test_cosmic)\n",
    "\n",
    "#no imputations yet, do this later\n",
    "X_train_tech = X_train[tech_cols]\n",
    "X_test_tech = X_test[tech_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7054de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ann = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularization term\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [1000, 2000],  # Increase max iterations\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "    'max_depth': [3, 6, 9],  # Maximum depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Proportion of data to sample\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],  # Subsample for each tree\n",
    "    'gamma': [0, 0.1, 0.2]  # Regularization term\n",
    "}\n",
    "\n",
    "param_grid_tree = {\n",
    "    'max_depth': [3, 5, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider for splits\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb08d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# GridSearchCV for each model\n",
    "grid_search_ann = GridSearchCV(estimator=mlp, param_grid=param_grid_ann, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_tree = GridSearchCV(estimator=tree, param_grid=param_grid_tree, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the models\n",
    "grid_search_ann.fit(X_train_cosmic, y_train)\n",
    "grid_search_xgb.fit(X_train_cosmic, y_train)\n",
    "grid_search_tree.fit(X_train_cosmic, y_train)\n",
    "\n",
    "# Get the best models\n",
    "best_mlp = grid_search_ann.best_estimator_\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "best_tree = grid_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98a49258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for ANN: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'max_iter': 2000, 'solver': 'adam'}\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best parameters for Decision Tree: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters found by GridSearchCV\n",
    "best_params_ann = grid_search_ann.best_params_\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_params_tree = grid_search_tree.best_params_\n",
    "\n",
    "print(\"Best parameters for ANN:\", best_params_ann)\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "print(\"Best parameters for Decision Tree:\", best_params_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP model: MLPRegressor(alpha=0.001, hidden_layer_sizes=(50, 50), max_iter=2000,\n",
      "             random_state=42)\n",
      "Best XGBoost model: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             feature_weights=None, gamma=0, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
      "             n_jobs=None, num_parallel_tree=None, ...)\n",
      "Best Decision Tree model: DecisionTreeRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=4,\n",
      "                      random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Access the best trained model (estimator)\n",
    "best_mlp = grid_search_ann.best_estimator_\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "best_tree_model = grid_search_tree.best_estimator_\n",
    "\n",
    "print(\"Best MLP model:\", best_mlp)\n",
    "print(\"Best XGBoost model:\", best_xgb_model)\n",
    "print(\"Best Decision Tree model:\", best_tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2c4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
