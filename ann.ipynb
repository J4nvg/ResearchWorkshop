{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:07.162471Z",
     "start_time": "2025-04-29T11:44:59.841948Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:17.105305Z",
     "start_time": "2025-04-29T11:45:11.857557Z"
    }
   },
   "cell_type": "code",
   "source": "ISBG = pd.read_excel(\"./data/ISBSG-whole.xlsx\",header=3)",
   "id": "fd991e6a879960a5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:17.330241Z",
     "start_time": "2025-04-29T11:45:17.318387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_needed = ['Max Team Size','COSMIC Read','COSMIC Write','COSMIC Entry','COSMIC Exit','Functional Size','Project Elapsed Time','Development Platform','Primary Programming Language','Summary Work Effort']\n",
    "ISBG_interest = ISBG[cols_needed]\n",
    "df_clean = ISBG_interest.dropna(subset=[\"COSMIC Read\", \"COSMIC Write\", \"COSMIC Exit\", \"COSMIC Entry\",\"Summary Work Effort\"])"
   ],
   "id": "2a1e4e6ca528812e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:19.853210Z",
     "start_time": "2025-04-29T11:45:19.849110Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4c1579b80ca1ce63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:20.731065Z",
     "start_time": "2025-04-29T11:45:20.415404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_clean = df_clean.drop(columns=[\"Summary Work Effort\"])\n",
    "y_clean = df_clean[\"Summary Work Effort\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tech_features = [\"Development Platform\", \"Primary Programming Language\",\n",
    "                 \"Project Elapsed Time\", \"Max Team Size\"]\n",
    "\n",
    "functional_features = [\"COSMIC Read\", \"COSMIC Write\", \"COSMIC Entry\", \"COSMIC Exit\",'Functional Size']\n",
    "\n",
    "# Split tech and func, training and test sets\n",
    "X_tech_train = X_train[tech_features]\n",
    "X_tech_test = X_test[tech_features]\n",
    "\n",
    "X_func_train = X_train[functional_features]\n",
    "X_func_test = X_test[functional_features]\n",
    "\n",
    "# Get numerical and categorical feature names\n",
    "tech_num_cols = X_tech_train.select_dtypes(include='number').columns\n",
    "tech_cat_cols = X_tech_train.select_dtypes(exclude='number').columns\n",
    "\n",
    "func_num_cols = X_func_train.select_dtypes(include='number').columns\n",
    "func_cat_cols = X_func_train.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Get right columns\n",
    "#Tech\n",
    "X_num_train_tech = X_tech_train[tech_num_cols]\n",
    "X_num_test_tech = X_tech_test[tech_num_cols]\n",
    "\n",
    "X_cat_train_tech = X_tech_train[tech_cat_cols]\n",
    "X_cat_test_tech = X_tech_test[tech_cat_cols]\n",
    "#func\n",
    "X_num_train_func = X_func_train[func_num_cols]\n",
    "X_num_test_func = X_func_test[func_num_cols]\n",
    "\n",
    "X_cat_train_func = X_func_train[func_cat_cols]\n",
    "X_cat_test_func = X_func_test[func_cat_cols]\n",
    "\n"
   ],
   "id": "729ca24b3acc8341",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:23.329256Z",
     "start_time": "2025-04-29T11:45:22.685096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(y_clean)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "sns.histplot(np.log1p(y_clean),kde=True)\n",
    "plt.show()"
   ],
   "id": "de50e58dda7aadea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:26.425768Z",
     "start_time": "2025-04-29T11:45:26.417273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming your DataFrame is named df_clean (or replace with your DataFrame variable)\n",
    "\n",
    "# 1. Get the count of missing values per column\n",
    "missing_counts = df_clean.isnull().sum()\n",
    "\n",
    "# 2. (Optional) Also get the percentage of missing values per column\n",
    "missing_pct = df_clean.isnull().mean() * 100\n",
    "\n",
    "# 3. Combine into a single DataFrame for a neat display\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': missing_counts,\n",
    "    'missing_pct': missing_pct\n",
    "})\n",
    "\n",
    "print(missing_summary)\n"
   ],
   "id": "23718501b76508da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              missing_count  missing_pct\n",
      "Max Team Size                            86    33.992095\n",
      "COSMIC Read                               0     0.000000\n",
      "COSMIC Write                              0     0.000000\n",
      "COSMIC Entry                              0     0.000000\n",
      "COSMIC Exit                               0     0.000000\n",
      "Functional Size                           0     0.000000\n",
      "Project Elapsed Time                     20     7.905138\n",
      "Development Platform                     60    23.715415\n",
      "Primary Programming Language             16     6.324111\n",
      "Summary Work Effort                       0     0.000000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:27.768659Z",
     "start_time": "2025-04-29T11:45:27.379195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "\n",
    "X_clean = df_clean.drop(columns=[\"Summary Work Effort\"])\n",
    "y_clean = df_clean[\"Summary Work Effort\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "all_numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "all_categorical_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "tech_features_orig = [\"Development Platform\", \"Primary Programming Language\",\n",
    "                      \"Project Elapsed Time\", \"Max Team Size\"]\n",
    "functional_features_orig = [\"COSMIC Read\", \"COSMIC Write\", \"COSMIC Entry\",\n",
    "                            \"COSMIC Exit\",'Functional Size']\n",
    "\n",
    "numeric_pipeline_steps = [\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('log1p', FunctionTransformer(np.log1p, validate=False, feature_names_out='one-to-one')),\n",
    "    ('scaler', StandardScaler())\n",
    "]\n",
    "numeric_pipeline = Pipeline(numeric_pipeline_steps)\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "tech_num_cols = [col for col in tech_features_orig if col in all_numeric_cols]\n",
    "tech_cat_cols = [col for col in tech_features_orig if col in all_categorical_cols]\n",
    "func_num_cols = [col for col in functional_features_orig if col in all_numeric_cols]\n",
    "func_cat_cols = [col for col in functional_features_orig if col in all_categorical_cols]\n",
    "\n",
    "numeric_cols_for_preprocessing = tech_num_cols + func_num_cols\n",
    "categorical_cols_for_preprocessing = tech_cat_cols + func_cat_cols\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_cols_for_preprocessing),\n",
    "        ('cat', categorical_pipeline, categorical_cols_for_preprocessing)\n",
    "    ],\n",
    ")\n",
    "\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names_out = preprocessor.get_feature_names_out()\n",
    "\n",
    "X_train_prepared_df = pd.DataFrame(X_train_prepared, columns=feature_names_out, index=X_train.index)\n",
    "X_test_prepared_df = pd.DataFrame(X_test_prepared, columns=feature_names_out, index=X_test.index)\n",
    "\n",
    "tech_prepared_cols = []\n",
    "func_prepared_cols = []\n",
    "\n",
    "\n",
    "# Thanks to chatgpt for this piece:\n",
    "for col_name in feature_names_out:\n",
    "    parts = col_name.split('__')\n",
    "    if len(parts) > 1:\n",
    "        original_feature = parts[1].split('_')[0] # Basic extraction, might need refinement if feature names contain underscores\n",
    "        # More robust extraction if needed:\n",
    "        # Check if original_feature exists in numeric/categorical lists before assigning\n",
    "        if parts[0] == 'num': # Check if it came from the numeric pipeline\n",
    "             # Find the original name (assuming it's the second part)\n",
    "             original_feature = parts[1]\n",
    "        elif parts[0] == 'cat': # Check if it came from the categorical pipeline\n",
    "             # OneHotEncoder adds the category value, find the base feature name\n",
    "             # Find the original feature name that this encoded column belongs to\n",
    "             original_feature = next((cat_col for cat_col in categorical_cols_for_preprocessing if col_name.startswith(f\"cat__{cat_col}_\")), None)\n",
    "             if original_feature is None and col_name.split('__')[1] in categorical_cols_for_preprocessing:\n",
    "                  # Handle cases where OHE might not add _category (e.g., binary features, drop='if_binary') - less common with sparse_output=False\n",
    "                  original_feature = col_name.split('__')[1]\n",
    "\n",
    "        if original_feature in tech_features_orig:\n",
    "            tech_prepared_cols.append(col_name)\n",
    "        elif original_feature in functional_features_orig:\n",
    "            func_prepared_cols.append(col_name)\n",
    "\n",
    "X_train_tech = X_train_prepared_df[tech_prepared_cols]\n",
    "X_test_tech = X_test_prepared_df[tech_prepared_cols]\n",
    "\n",
    "X_train_func = X_train_prepared_df[func_prepared_cols]\n",
    "X_test_func = X_test_prepared_df[func_prepared_cols]\n",
    "\n",
    "print(\"\\n Separate datasets for tech and func features:\")\n",
    "print(\"Shape of X_train_tech:\", X_train_tech.shape)\n",
    "print(\"Shape of X_test_tech:\", X_test_tech.shape)\n",
    "print(\"Shape of X_train_func:\", X_train_func.shape)\n",
    "print(\"Shape of X_test_func:\", X_test_func.shape)\n"
   ],
   "id": "ffb9997f5d925c8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Separate datasets for tech and func features:\n",
      "Shape of X_train_tech: (202, 25)\n",
      "Shape of X_test_tech: (51, 25)\n",
      "Shape of X_train_func: (202, 5)\n",
      "Shape of X_test_func: (51, 5)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:45:31.440932Z",
     "start_time": "2025-04-29T11:45:30.640717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost as xgboost\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "print(f\"True mean of SWE {df_clean['Summary Work Effort'].mean()} standard_deviation SWE {df_clean['Summary Work Effort'].std()}\\n\")\n",
    "def evaluate_model(model_name,y_true,y_pred_log):\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    # y_pred = np.max(0,y_pred)\n",
    "    mse = mean_squared_error(y_true,y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true,y_pred)\n",
    "    mae = mean_absolute_error(y_true,y_pred)\n",
    "    print(f\"\\nModel: {model_name}, mse : {mse}, r2 : {r2}, mae : {mae}, rmse {rmse}\")\n",
    "    return mse,r2,mae\n",
    "\n",
    "DTR = DecisionTreeRegressor(random_state=42)\n",
    "dtr_model_tech = DTR.fit(X_train_tech,y_train_log)\n",
    "dtr_pred_log = dtr_model_tech.predict(X_test_tech)\n",
    "results_dtr_tech = evaluate_model(\"DTR_tech\",y_test,dtr_pred_log)\n",
    "\n",
    "dtr_model_func = DTR.fit(X_train_func,y_train_log)\n",
    "dtr_pred_log = dtr_model_func.predict(X_test_func)\n",
    "results_dtr_func = evaluate_model(\"DTR_func\",y_test,dtr_pred_log)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(objective=\"reg:squarederror\",random_state=42)\n",
    "xgb_model_tech = xgb.fit(X_train_tech,y_train_log)\n",
    "xgb_pred_log = xgb_model_tech.predict(X_test_tech)\n",
    "results_xgb_tech = evaluate_model(\"XGB_tech\",y_test,xgb_pred_log)\n",
    "\n",
    "xgb_model_func = xgb.fit(X_train_func,y_train_log)\n",
    "xgb_pred_log = xgb_model_func.predict(X_test_func)\n",
    "results_xgb_func = evaluate_model(\"XGB_func\",y_test,xgb_pred_log)\n",
    "\n"
   ],
   "id": "7535b5c5253f714",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean of SWE 2545.9723320158105 standard_deviation SWE 5514.492199421938\n",
      "\n",
      "\n",
      "Model: DTR_tech, mse : 37931350.105368115, r2 : 0.2335303831862876, mae : 2113.089172304204, rmse 6158.8432440977185\n",
      "\n",
      "Model: DTR_func, mse : 45335792.2415504, r2 : 0.08391061191334892, mae : 2279.092163519397, rmse 6733.185890910067\n",
      "\n",
      "Model: XGB_tech, mse : 38975909.55907181, r2 : 0.2124232229614239, mae : 1906.210646498437, rmse 6243.0689215378525\n",
      "\n",
      "Model: XGB_func, mse : 48923715.74191453, r2 : 0.011410309581852851, mae : 2393.632090250651, rmse 6994.549002038269\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:57:45.442561300Z",
     "start_time": "2025-04-27T15:30:37.899746Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9178e7e9049d01f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:46:03.768335Z",
     "start_time": "2025-04-29T11:45:41.305710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- 1) Define param grids ---\n",
    "param_grid_dtr = {\n",
    "    'max_depth':      [None, 3,4, 5,6,7,8,9, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf':  [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators':   [100, 200, 300,400,500],\n",
    "    'max_depth':      [3, 5, 7],\n",
    "    'learning_rate':  [0.01, 0.1, 0.2],\n",
    "    'subsample':      [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# --- 2) Set up GridSearchCV for each model / feature set ---\n",
    "# Decision Tree on tech features\n",
    "grid_dtr_tech = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=param_grid_dtr,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_dtr_tech.fit(X_train_tech, y_train_log)\n",
    "\n",
    "# Decision Tree on func features\n",
    "grid_dtr_func = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=param_grid_dtr,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_dtr_func.fit(X_train_func, y_train_log)\n",
    "\n",
    "# XGB on tech features\n",
    "grid_xgb_tech = GridSearchCV(\n",
    "    xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_xgb_tech.fit(X_train_tech, y_train_log)\n",
    "\n",
    "# XGB on func features\n",
    "grid_xgb_func = GridSearchCV(\n",
    "    xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_xgb_func.fit(X_train_func, y_train_log)\n",
    "\n",
    "# --- 3) Inspect best params ---\n",
    "print(\"Best DTR tech:\", grid_dtr_tech.best_params_)\n",
    "print(\"Best DTR func:\", grid_dtr_func.best_params_)\n",
    "print(\"Best XGB tech:\", grid_xgb_tech.best_params_)\n",
    "print(\"Best XGB func:\", grid_xgb_func.best_params_)\n",
    "\n",
    "# --- 4) Evaluate on the hold-out test set ---\n",
    "best_dtr_tech = grid_dtr_tech.best_estimator_\n",
    "best_dtr_func = grid_dtr_func.best_estimator_\n",
    "best_xgb_tech = grid_xgb_tech.best_estimator_\n",
    "best_xgb_func = grid_xgb_func.best_estimator_\n",
    "\n",
    "dtr_tech_pred = best_dtr_tech.predict(X_test_tech)\n",
    "dtr_func_pred = best_dtr_func.predict(X_test_func)\n",
    "xgb_tech_pred = best_xgb_tech.predict(X_test_tech)\n",
    "xgb_func_pred = best_xgb_func.predict(X_test_func)\n",
    "\n",
    "print(\"\\n=== Final test performance (after grid search) ===\")\n",
    "evaluate_model(\"DTR_tech (GS)\", y_test, dtr_tech_pred)\n",
    "evaluate_model(\"DTR_func (GS)\", y_test, dtr_func_pred)\n",
    "evaluate_model(\"XGB_tech (GS)\", y_test, xgb_tech_pred)\n",
    "evaluate_model(\"XGB_func (GS)\", y_test, xgb_func_pred)\n"
   ],
   "id": "a0a785f78cca5fa5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "3 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1222, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "    ^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "           ^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [13:45:50] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Janvg\\miniconda3\\envs\\ResearchWorkshop\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan -1.29670844 -1.09067398 -1.15696334\n",
      " -1.02521781 -1.09574913 -0.99276755 -1.07051337 -1.66004276 -1.74369507\n",
      " -1.16436831 -1.26426207 -1.04603744 -1.17117955 -1.01363302 -1.13941068\n",
      " -1.00583557 -1.13065395 -1.67062407 -1.7589045  -1.19309174 -1.29008414\n",
      " -1.08939957 -1.2161881  -1.06827376 -1.21415759 -1.07143033 -1.22124851\n",
      " -0.92041999 -0.99785016 -0.93003717 -0.97490813 -0.95085003 -0.97957162\n",
      " -0.97091628 -1.01227942 -0.98865658 -1.03050609 -1.06319337 -1.12459468\n",
      " -1.11410599 -1.16924885 -1.12241273 -1.2108433  -1.12710737 -1.23102882\n",
      " -1.13130238 -1.23903662 -1.1155843  -1.26031776 -1.15020053 -1.29032083\n",
      " -1.15177235 -1.29768004 -1.14997385 -1.29799849 -1.15096987 -1.29799851\n",
      " -0.97168952 -0.96564846 -1.01610753 -1.0164712  -1.0312982  -1.06259748\n",
      " -1.03114516 -1.10471354 -1.04922551 -1.12428132 -1.10015806 -1.12851028\n",
      " -1.13741091 -1.17731333 -1.13027836 -1.18693869 -1.12688101 -1.18784497\n",
      " -1.12911687 -1.18784498 -1.15998125 -1.28115727 -1.17065086 -1.28716257\n",
      " -1.17134393 -1.28716307 -1.16594249 -1.28716315 -1.16331525 -1.28716315]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DTR tech: {'max_depth': 7, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best DTR func: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best XGB tech: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best XGB func: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}\n",
      "\n",
      "=== Final test performance (after grid search) ===\n",
      "\n",
      "Model: DTR_tech (GS), mse : 37388643.34262642, r2 : 0.24449672747209172, mae : 1847.57520409642, rmse 6114.625364045325\n",
      "\n",
      "Model: DTR_func (GS), mse : 42696594.522427104, r2 : 0.1372402418593649, mae : 2347.112982465331, rmse 6534.263120079196\n",
      "\n",
      "Model: XGB_tech (GS), mse : 38770120.982825756, r2 : 0.21658154294586407, mae : 1964.8140569574693, rmse 6226.56574548328\n",
      "\n",
      "Model: XGB_func (GS), mse : 45341274.26527108, r2 : 0.08379983798600688, mae : 2376.1983079349293, rmse 6733.592968488004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45341274.26527108, 0.08379983798600688, 2376.1983079349293)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T11:47:13.709543Z",
     "start_time": "2025-04-29T11:46:15.753472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras._tf_keras.keras.losses import huber\n",
    "\n",
    "## TECH ##\n",
    "X_train_keras_tech = X_train_tech.values.astype(np.float32)\n",
    "X_test_keras_tech = X_test_tech.values.astype(np.float32)\n",
    "\n",
    "## FUNC ##\n",
    "X_train_keras_func = X_train_func.values.astype(np.float32)\n",
    "X_test_keras_func = X_test_func.values.astype(np.float32)\n",
    "\n",
    "## BOTH ##\n",
    "\n",
    "X_train_keras_combined = pd.concat([X_train_tech, X_train_func], axis=1).values.astype(np.float32)\n",
    "X_test_keras_combined = pd.concat([X_test_tech, X_test_func], axis=1).values.astype(np.float32)\n",
    "\n",
    "y_train_log_keras = y_train_log.astype(np.float32)\n",
    "\n",
    "input_shape_tech = (X_train_keras_tech.shape[1],)\n",
    "input_shape_func = (X_train_keras_func.shape[1],)\n",
    "input_shape_comb = (X_train_keras_combined.shape[1],)\n",
    "\n",
    "# Define the model TECH\n",
    "keras_model_tech = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=input_shape_tech),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3), # Add dropout for regularization\n",
    "        # layers.Dense(64, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model FUNC\n",
    "keras_model_func = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=input_shape_func),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # layers.Dropout(0.1), # Add dropout for regularization\n",
    "        # layers.Dense(128, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        # layers.Dense(128, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "# Define the model FUNC\n",
    "keras_model_comb = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=input_shape_comb),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        # layers.Dropout(0.1), # Add dropout for regularization\n",
    "        # layers.Dense(128, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        # layers.Dense(128, activation=\"relu\"),\n",
    "        # layers.Dropout(0.3),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "keras_model_tech.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001), # Adam optimizer\n",
    "    loss=huber,  # Good tradeoff between MAE and MSE\n",
    ")\n",
    "# Compile the model\n",
    "keras_model_func.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=huber,\n",
    ")\n",
    "# Compile the model\n",
    "keras_model_comb.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=huber,\n",
    ")\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_loss', # Monitor validation loss\n",
    "    patience=20,        # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True, # Restore model weights from the epoch with the best value\n",
    "    verbose=1\n",
    ")\n",
    "#\n",
    "# print(\"Keras TECH Model Summary:\")\n",
    "# keras_model_tech.summary()\n",
    "#\n",
    "# print(\"Keras FUNC Model Summary:\")\n",
    "# keras_model_func.summary()\n",
    "\n",
    "# Train the TECH model\n",
    "history_tech = keras_model_tech.fit(\n",
    "    X_train_keras_tech,\n",
    "    y_train_log_keras,\n",
    "    epochs=150, # Max number of epochs\n",
    "    batch_size=16,\n",
    "    validation_split=0.15, # Use 15% of training data for validation\n",
    "    callbacks=[early_stopping_cb],\n",
    "    verbose=1, # Set to 1 or 2 to see epoch progress\n",
    ")\n",
    "# # Train the FUNC model\n",
    "history_func = keras_model_func.fit(\n",
    "    X_train_keras_func,\n",
    "    y_train_log_keras,\n",
    "    epochs=150, # Max number of epochs\n",
    "    batch_size=16,\n",
    "    validation_split=0.15, # Use 15% of training data for validation\n",
    "    callbacks=[early_stopping_cb],\n",
    "    verbose=1, # Set to 1 or 2 to see epoch progress\n",
    ")\n",
    "# # Train the FUNC model\n",
    "history_comb = keras_model_comb.fit(\n",
    "    X_train_keras_combined,\n",
    "    y_train_log_keras,\n",
    "    epochs=150, # Max number of epochs\n",
    "    batch_size=16,\n",
    "    validation_split=0.15, # Use 15% of training data for validation\n",
    "    callbacks=[early_stopping_cb],\n",
    "    verbose=1, # Set to 1 or 2 to see epoch progress\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the TECH model\n",
    "keras_preds_log_tech = keras_model_tech.predict(X_test_keras_tech).flatten() # Predict and flatten output\n",
    "\n",
    "\n",
    "# Evaluate the TECH model\n",
    "keras_preds_log_comb = keras_model_comb.predict(X_test_keras_combined).flatten() # Predict and flatten output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the TECH model\n",
    "keras_preds_log_func = keras_model_func.predict(X_test_keras_func).flatten() # Predict and flatten output\n",
    "print(f\"Keras FUNC training stopped after {len(history_func.history['loss'])} epochs.\")\n",
    "evaluate_model(f\"MLP_func\", y_test, keras_preds_log_func)\n",
    "print(f\"Keras TECH training stopped after {len(history_tech.history['loss'])} epochs.\")\n",
    "evaluate_model(f\"MLP_tech\", y_test, keras_preds_log_tech)\n",
    "print(f\"Keras COMB training stopped after {len(history_tech.history['loss'])} epochs.\")\n",
    "evaluate_model(f\"MLP_comb\", y_test, keras_preds_log_comb)\n"
   ],
   "id": "e3f777e0ab077830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 31ms/step - loss: 5.4783 - val_loss: 5.5086\n",
      "Epoch 2/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 5.1358 - val_loss: 5.3007\n",
      "Epoch 3/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 5.2565 - val_loss: 5.0965\n",
      "Epoch 4/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 5.0652 - val_loss: 4.8883\n",
      "Epoch 5/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 4.8598 - val_loss: 4.6788\n",
      "Epoch 6/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 4.2626 - val_loss: 4.4598\n",
      "Epoch 7/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 4.1603 - val_loss: 4.2363\n",
      "Epoch 8/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 4.2329 - val_loss: 4.0104\n",
      "Epoch 9/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 3.7588 - val_loss: 3.7763\n",
      "Epoch 10/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 3.8127 - val_loss: 3.5352\n",
      "Epoch 11/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 3.4971 - val_loss: 3.2870\n",
      "Epoch 12/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 3.2346 - val_loss: 3.0300\n",
      "Epoch 13/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.9358 - val_loss: 2.7659\n",
      "Epoch 14/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.7147 - val_loss: 2.5035\n",
      "Epoch 15/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.2406 - val_loss: 2.2595\n",
      "Epoch 16/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.2169 - val_loss: 2.0262\n",
      "Epoch 17/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 1.9101 - val_loss: 1.8144\n",
      "Epoch 18/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 1.5147 - val_loss: 1.6161\n",
      "Epoch 19/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 1.4551 - val_loss: 1.4233\n",
      "Epoch 20/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 1.4249 - val_loss: 1.2440\n",
      "Epoch 21/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.3230 - val_loss: 1.0806\n",
      "Epoch 22/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 1.0474 - val_loss: 0.9449\n",
      "Epoch 23/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 1.0479 - val_loss: 0.8270\n",
      "Epoch 24/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.7974 - val_loss: 0.7309\n",
      "Epoch 25/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.8132 - val_loss: 0.6492\n",
      "Epoch 26/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.7305 - val_loss: 0.5812\n",
      "Epoch 27/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6177 - val_loss: 0.5419\n",
      "Epoch 28/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.6639 - val_loss: 0.5092\n",
      "Epoch 29/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6725 - val_loss: 0.4874\n",
      "Epoch 30/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.5955 - val_loss: 0.4741\n",
      "Epoch 31/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6217 - val_loss: 0.4615\n",
      "Epoch 32/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6272 - val_loss: 0.4548\n",
      "Epoch 33/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.5480 - val_loss: 0.4499\n",
      "Epoch 34/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5921 - val_loss: 0.4424\n",
      "Epoch 35/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5166 - val_loss: 0.4371\n",
      "Epoch 36/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5159 - val_loss: 0.4318\n",
      "Epoch 37/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5451 - val_loss: 0.4285\n",
      "Epoch 38/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5715 - val_loss: 0.4255\n",
      "Epoch 39/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4846 - val_loss: 0.4217\n",
      "Epoch 40/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.5019 - val_loss: 0.4158\n",
      "Epoch 41/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5590 - val_loss: 0.4130\n",
      "Epoch 42/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5793 - val_loss: 0.4068\n",
      "Epoch 43/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4650 - val_loss: 0.4042\n",
      "Epoch 44/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4984 - val_loss: 0.4026\n",
      "Epoch 45/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.5159 - val_loss: 0.4017\n",
      "Epoch 46/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4821 - val_loss: 0.3981\n",
      "Epoch 47/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5319 - val_loss: 0.3949\n",
      "Epoch 48/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5222 - val_loss: 0.3927\n",
      "Epoch 49/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5351 - val_loss: 0.3913\n",
      "Epoch 50/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4785 - val_loss: 0.3868\n",
      "Epoch 51/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4926 - val_loss: 0.3870\n",
      "Epoch 52/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5111 - val_loss: 0.3819\n",
      "Epoch 53/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3895 - val_loss: 0.3769\n",
      "Epoch 54/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4325 - val_loss: 0.3752\n",
      "Epoch 55/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4233 - val_loss: 0.3738\n",
      "Epoch 56/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4705 - val_loss: 0.3714\n",
      "Epoch 57/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4277 - val_loss: 0.3696\n",
      "Epoch 58/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4212 - val_loss: 0.3673\n",
      "Epoch 59/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4062 - val_loss: 0.3632\n",
      "Epoch 60/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4046 - val_loss: 0.3617\n",
      "Epoch 61/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.5425 - val_loss: 0.3620\n",
      "Epoch 62/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3973 - val_loss: 0.3596\n",
      "Epoch 63/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.4208 - val_loss: 0.3575\n",
      "Epoch 64/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 0.4174 - val_loss: 0.3552\n",
      "Epoch 65/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4525 - val_loss: 0.3528\n",
      "Epoch 66/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.4101 - val_loss: 0.3536\n",
      "Epoch 67/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4002 - val_loss: 0.3502\n",
      "Epoch 68/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4393 - val_loss: 0.3485\n",
      "Epoch 69/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3723 - val_loss: 0.3468\n",
      "Epoch 70/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3988 - val_loss: 0.3446\n",
      "Epoch 71/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4029 - val_loss: 0.3428\n",
      "Epoch 72/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3501 - val_loss: 0.3428\n",
      "Epoch 73/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3573 - val_loss: 0.3384\n",
      "Epoch 74/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4092 - val_loss: 0.3394\n",
      "Epoch 75/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.4061 - val_loss: 0.3409\n",
      "Epoch 76/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3898 - val_loss: 0.3368\n",
      "Epoch 77/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3795 - val_loss: 0.3350\n",
      "Epoch 78/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3856 - val_loss: 0.3327\n",
      "Epoch 79/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4122 - val_loss: 0.3300\n",
      "Epoch 80/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3865 - val_loss: 0.3291\n",
      "Epoch 81/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3890 - val_loss: 0.3254\n",
      "Epoch 82/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3856 - val_loss: 0.3243\n",
      "Epoch 83/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4150 - val_loss: 0.3229\n",
      "Epoch 84/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4116 - val_loss: 0.3255\n",
      "Epoch 85/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3816 - val_loss: 0.3237\n",
      "Epoch 86/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4092 - val_loss: 0.3229\n",
      "Epoch 87/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3364 - val_loss: 0.3207\n",
      "Epoch 88/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3778 - val_loss: 0.3180\n",
      "Epoch 89/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3211 - val_loss: 0.3185\n",
      "Epoch 90/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3958 - val_loss: 0.3181\n",
      "Epoch 91/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3336 - val_loss: 0.3169\n",
      "Epoch 92/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3094 - val_loss: 0.3172\n",
      "Epoch 93/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2536 - val_loss: 0.3166\n",
      "Epoch 94/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3625 - val_loss: 0.3156\n",
      "Epoch 95/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3103 - val_loss: 0.3142\n",
      "Epoch 96/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2833 - val_loss: 0.3094\n",
      "Epoch 97/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3478 - val_loss: 0.3119\n",
      "Epoch 98/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2968 - val_loss: 0.3145\n",
      "Epoch 99/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3237 - val_loss: 0.3124\n",
      "Epoch 100/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3644 - val_loss: 0.3167\n",
      "Epoch 101/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3691 - val_loss: 0.3098\n",
      "Epoch 102/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3071 - val_loss: 0.3102\n",
      "Epoch 103/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2910 - val_loss: 0.3089\n",
      "Epoch 104/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3346 - val_loss: 0.3095\n",
      "Epoch 105/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4003 - val_loss: 0.3085\n",
      "Epoch 106/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3067 - val_loss: 0.3085\n",
      "Epoch 107/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2839 - val_loss: 0.3095\n",
      "Epoch 108/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3271 - val_loss: 0.3086\n",
      "Epoch 109/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3174 - val_loss: 0.3063\n",
      "Epoch 110/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3203 - val_loss: 0.3064\n",
      "Epoch 111/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3735 - val_loss: 0.3085\n",
      "Epoch 112/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3073 - val_loss: 0.3075\n",
      "Epoch 113/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3460 - val_loss: 0.3058\n",
      "Epoch 114/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3336 - val_loss: 0.3043\n",
      "Epoch 115/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3569 - val_loss: 0.3051\n",
      "Epoch 116/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2990 - val_loss: 0.3046\n",
      "Epoch 117/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3360 - val_loss: 0.3034\n",
      "Epoch 118/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2755 - val_loss: 0.3029\n",
      "Epoch 119/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2605 - val_loss: 0.3040\n",
      "Epoch 120/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3054 - val_loss: 0.3049\n",
      "Epoch 121/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3203 - val_loss: 0.3016\n",
      "Epoch 122/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3370 - val_loss: 0.3035\n",
      "Epoch 123/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3487 - val_loss: 0.3018\n",
      "Epoch 124/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3153 - val_loss: 0.3052\n",
      "Epoch 125/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3297 - val_loss: 0.3034\n",
      "Epoch 126/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2740 - val_loss: 0.3035\n",
      "Epoch 127/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2872 - val_loss: 0.3017\n",
      "Epoch 128/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3157 - val_loss: 0.3027\n",
      "Epoch 129/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2885 - val_loss: 0.3017\n",
      "Epoch 130/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2712 - val_loss: 0.2999\n",
      "Epoch 131/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3178 - val_loss: 0.2997\n",
      "Epoch 132/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3021 - val_loss: 0.2974\n",
      "Epoch 133/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2527 - val_loss: 0.3008\n",
      "Epoch 134/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2519 - val_loss: 0.2988\n",
      "Epoch 135/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3379 - val_loss: 0.2978\n",
      "Epoch 136/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2784 - val_loss: 0.3030\n",
      "Epoch 137/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3078 - val_loss: 0.2981\n",
      "Epoch 138/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2771 - val_loss: 0.2934\n",
      "Epoch 139/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2780 - val_loss: 0.2995\n",
      "Epoch 140/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3301 - val_loss: 0.2983\n",
      "Epoch 141/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2958 - val_loss: 0.2983\n",
      "Epoch 142/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3367 - val_loss: 0.2984\n",
      "Epoch 143/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2694 - val_loss: 0.2991\n",
      "Epoch 144/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3333 - val_loss: 0.2971\n",
      "Epoch 145/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3034 - val_loss: 0.2970\n",
      "Epoch 146/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2624 - val_loss: 0.2952\n",
      "Epoch 147/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2872 - val_loss: 0.2965\n",
      "Epoch 148/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2919 - val_loss: 0.2950\n",
      "Epoch 149/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3296 - val_loss: 0.2924\n",
      "Epoch 150/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2805 - val_loss: 0.2948\n",
      "Restoring model weights from the end of the best epoch: 149.\n",
      "Epoch 1/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 20ms/step - loss: 5.4236 - val_loss: 5.5562\n",
      "Epoch 2/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 5.2915 - val_loss: 5.3449\n",
      "Epoch 3/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 5.0785 - val_loss: 5.1403\n",
      "Epoch 4/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 4.9013 - val_loss: 4.9384\n",
      "Epoch 5/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 4.7665 - val_loss: 4.7369\n",
      "Epoch 6/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 4.6432 - val_loss: 4.5353\n",
      "Epoch 7/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 4.4430 - val_loss: 4.3270\n",
      "Epoch 8/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 4.0739 - val_loss: 4.1067\n",
      "Epoch 9/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 3.8160 - val_loss: 3.8750\n",
      "Epoch 10/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 3.6753 - val_loss: 3.6368\n",
      "Epoch 11/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 3.3803 - val_loss: 3.3876\n",
      "Epoch 12/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 3.4026 - val_loss: 3.1601\n",
      "Epoch 13/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 3.0515 - val_loss: 2.9801\n",
      "Epoch 14/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 2.9914 - val_loss: 2.8428\n",
      "Epoch 15/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 2.6666 - val_loss: 2.7145\n",
      "Epoch 16/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 2.6823 - val_loss: 2.5975\n",
      "Epoch 17/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 2.8050 - val_loss: 2.4716\n",
      "Epoch 18/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 2.5541 - val_loss: 2.3523\n",
      "Epoch 19/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 2.4923 - val_loss: 2.2308\n",
      "Epoch 20/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 2.4460 - val_loss: 2.1142\n",
      "Epoch 21/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.2019 - val_loss: 1.9984\n",
      "Epoch 22/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 2.3082 - val_loss: 1.8963\n",
      "Epoch 23/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 2.1080 - val_loss: 1.8019\n",
      "Epoch 24/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.8799 - val_loss: 1.7219\n",
      "Epoch 25/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.9573 - val_loss: 1.6603\n",
      "Epoch 26/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.7615 - val_loss: 1.6090\n",
      "Epoch 27/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.9024 - val_loss: 1.5723\n",
      "Epoch 28/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.8215 - val_loss: 1.5388\n",
      "Epoch 29/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.7602 - val_loss: 1.5108\n",
      "Epoch 30/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.7374 - val_loss: 1.4783\n",
      "Epoch 31/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.7206 - val_loss: 1.4524\n",
      "Epoch 32/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.6730 - val_loss: 1.4349\n",
      "Epoch 33/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.6332 - val_loss: 1.4052\n",
      "Epoch 34/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.4677 - val_loss: 1.3815\n",
      "Epoch 35/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.5442 - val_loss: 1.3579\n",
      "Epoch 36/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.4532 - val_loss: 1.3274\n",
      "Epoch 37/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.3099 - val_loss: 1.3003\n",
      "Epoch 38/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.4511 - val_loss: 1.2706\n",
      "Epoch 39/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.4479 - val_loss: 1.2392\n",
      "Epoch 40/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 1.2552 - val_loss: 1.2069\n",
      "Epoch 41/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 1.3321 - val_loss: 1.1741\n",
      "Epoch 42/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.3441 - val_loss: 1.1408\n",
      "Epoch 43/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.2170 - val_loss: 1.1070\n",
      "Epoch 44/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.2002 - val_loss: 1.0748\n",
      "Epoch 45/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 1.2396 - val_loss: 1.0377\n",
      "Epoch 46/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 1.1996 - val_loss: 1.0033\n",
      "Epoch 47/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 1.0315 - val_loss: 0.9653\n",
      "Epoch 48/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 1.0193 - val_loss: 0.9250\n",
      "Epoch 49/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 1.0703 - val_loss: 0.8861\n",
      "Epoch 50/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.9281 - val_loss: 0.8428\n",
      "Epoch 51/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.9136 - val_loss: 0.8176\n",
      "Epoch 52/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.9403 - val_loss: 0.7818\n",
      "Epoch 53/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.8156 - val_loss: 0.7491\n",
      "Epoch 54/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.7489 - val_loss: 0.7170\n",
      "Epoch 55/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.8163 - val_loss: 0.6901\n",
      "Epoch 56/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.8161 - val_loss: 0.6685\n",
      "Epoch 57/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.7585 - val_loss: 0.6412\n",
      "Epoch 58/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.7348 - val_loss: 0.6269\n",
      "Epoch 59/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6280 - val_loss: 0.6014\n",
      "Epoch 60/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6643 - val_loss: 0.5936\n",
      "Epoch 61/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.6178 - val_loss: 0.5808\n",
      "Epoch 62/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6487 - val_loss: 0.5708\n",
      "Epoch 63/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6124 - val_loss: 0.5620\n",
      "Epoch 64/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 0.6551 - val_loss: 0.5530\n",
      "Epoch 65/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5882 - val_loss: 0.5468\n",
      "Epoch 66/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5792 - val_loss: 0.5475\n",
      "Epoch 67/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5405 - val_loss: 0.5426\n",
      "Epoch 68/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5828 - val_loss: 0.5399\n",
      "Epoch 69/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.6193 - val_loss: 0.5398\n",
      "Epoch 70/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5593 - val_loss: 0.5368\n",
      "Epoch 71/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6246 - val_loss: 0.5373\n",
      "Epoch 72/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.6168 - val_loss: 0.5396\n",
      "Epoch 73/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5165 - val_loss: 0.5378\n",
      "Epoch 74/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5378 - val_loss: 0.5391\n",
      "Epoch 75/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5349 - val_loss: 0.5400\n",
      "Epoch 76/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5248 - val_loss: 0.5409\n",
      "Epoch 77/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5775 - val_loss: 0.5427\n",
      "Epoch 78/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.5467 - val_loss: 0.5419\n",
      "Epoch 79/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4962 - val_loss: 0.5427\n",
      "Epoch 80/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5178 - val_loss: 0.5444\n",
      "Epoch 81/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5021 - val_loss: 0.5429\n",
      "Epoch 82/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5908 - val_loss: 0.5453\n",
      "Epoch 83/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6236 - val_loss: 0.5451\n",
      "Epoch 84/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5445 - val_loss: 0.5461\n",
      "Epoch 85/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5241 - val_loss: 0.5470\n",
      "Epoch 86/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5669 - val_loss: 0.5466\n",
      "Epoch 87/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5966 - val_loss: 0.5468\n",
      "Epoch 88/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6056 - val_loss: 0.5495\n",
      "Epoch 89/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5103 - val_loss: 0.5485\n",
      "Epoch 90/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5545 - val_loss: 0.5489\n",
      "Epoch 90: early stopping\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "Epoch 1/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 22ms/step - loss: 5.1629 - val_loss: 5.2378\n",
      "Epoch 2/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 5.1581 - val_loss: 4.9896\n",
      "Epoch 3/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 4.8481 - val_loss: 4.7354\n",
      "Epoch 4/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 4.7410 - val_loss: 4.4784\n",
      "Epoch 5/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 4.4709 - val_loss: 4.2137\n",
      "Epoch 6/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 4.0678 - val_loss: 3.9363\n",
      "Epoch 7/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 3.7603 - val_loss: 3.6421\n",
      "Epoch 8/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 3.6078 - val_loss: 3.3387\n",
      "Epoch 9/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 3.2917 - val_loss: 3.0307\n",
      "Epoch 10/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 3.0357 - val_loss: 2.7130\n",
      "Epoch 11/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.7389 - val_loss: 2.3832\n",
      "Epoch 12/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 2.1650 - val_loss: 2.0873\n",
      "Epoch 13/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 2.0123 - val_loss: 1.8223\n",
      "Epoch 14/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.6666 - val_loss: 1.5928\n",
      "Epoch 15/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 1.5419 - val_loss: 1.4069\n",
      "Epoch 16/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 1.4664 - val_loss: 1.2621\n",
      "Epoch 17/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 1.2337 - val_loss: 1.1740\n",
      "Epoch 18/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 1.1044 - val_loss: 1.1108\n",
      "Epoch 19/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 1.0985 - val_loss: 1.0734\n",
      "Epoch 20/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 1.0717 - val_loss: 1.0281\n",
      "Epoch 21/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 1.0615 - val_loss: 0.9923\n",
      "Epoch 22/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 1.0274 - val_loss: 0.9563\n",
      "Epoch 23/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.8987 - val_loss: 0.9202\n",
      "Epoch 24/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.9460 - val_loss: 0.8841\n",
      "Epoch 25/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.9045 - val_loss: 0.8524\n",
      "Epoch 26/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.8901 - val_loss: 0.8197\n",
      "Epoch 27/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.8335 - val_loss: 0.7850\n",
      "Epoch 28/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.9171 - val_loss: 0.7526\n",
      "Epoch 29/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.7533 - val_loss: 0.7217\n",
      "Epoch 30/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.7590 - val_loss: 0.6914\n",
      "Epoch 31/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.7028 - val_loss: 0.6673\n",
      "Epoch 32/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6572 - val_loss: 0.6402\n",
      "Epoch 33/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6084 - val_loss: 0.6173\n",
      "Epoch 34/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5735 - val_loss: 0.5923\n",
      "Epoch 35/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.6132 - val_loss: 0.5687\n",
      "Epoch 36/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5852 - val_loss: 0.5482\n",
      "Epoch 37/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5181 - val_loss: 0.5272\n",
      "Epoch 38/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5385 - val_loss: 0.5069\n",
      "Epoch 39/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5011 - val_loss: 0.4833\n",
      "Epoch 40/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5090 - val_loss: 0.4698\n",
      "Epoch 41/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5029 - val_loss: 0.4569\n",
      "Epoch 42/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4712 - val_loss: 0.4424\n",
      "Epoch 43/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4869 - val_loss: 0.4301\n",
      "Epoch 44/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4628 - val_loss: 0.4159\n",
      "Epoch 45/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4126 - val_loss: 0.4062\n",
      "Epoch 46/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4062 - val_loss: 0.3902\n",
      "Epoch 47/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4635 - val_loss: 0.3855\n",
      "Epoch 48/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3603 - val_loss: 0.3756\n",
      "Epoch 49/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3820 - val_loss: 0.3679\n",
      "Epoch 50/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3685 - val_loss: 0.3621\n",
      "Epoch 51/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3863 - val_loss: 0.3556\n",
      "Epoch 52/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3440 - val_loss: 0.3531\n",
      "Epoch 53/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3129 - val_loss: 0.3409\n",
      "Epoch 54/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3940 - val_loss: 0.3331\n",
      "Epoch 55/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3823 - val_loss: 0.3357\n",
      "Epoch 56/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3967 - val_loss: 0.3352\n",
      "Epoch 57/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2949 - val_loss: 0.3291\n",
      "Epoch 58/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2949 - val_loss: 0.3173\n",
      "Epoch 59/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3577 - val_loss: 0.3168\n",
      "Epoch 60/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3383 - val_loss: 0.3151\n",
      "Epoch 61/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3667 - val_loss: 0.3074\n",
      "Epoch 62/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3017 - val_loss: 0.3064\n",
      "Epoch 63/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2996 - val_loss: 0.3043\n",
      "Epoch 64/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3000 - val_loss: 0.3053\n",
      "Epoch 65/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3030 - val_loss: 0.2994\n",
      "Epoch 66/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2717 - val_loss: 0.2963\n",
      "Epoch 67/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2887 - val_loss: 0.2928\n",
      "Epoch 68/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2891 - val_loss: 0.2908\n",
      "Epoch 69/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.3359 - val_loss: 0.2911\n",
      "Epoch 70/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2703 - val_loss: 0.2865\n",
      "Epoch 71/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.3284 - val_loss: 0.2906\n",
      "Epoch 72/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3320 - val_loss: 0.2856\n",
      "Epoch 73/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2868 - val_loss: 0.2886\n",
      "Epoch 74/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2756 - val_loss: 0.2806\n",
      "Epoch 75/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3398 - val_loss: 0.2792\n",
      "Epoch 76/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3130 - val_loss: 0.2839\n",
      "Epoch 77/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2230 - val_loss: 0.2826\n",
      "Epoch 78/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3028 - val_loss: 0.2799\n",
      "Epoch 79/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2851 - val_loss: 0.2756\n",
      "Epoch 80/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3101 - val_loss: 0.2833\n",
      "Epoch 81/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2704 - val_loss: 0.2776\n",
      "Epoch 82/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2864 - val_loss: 0.2755\n",
      "Epoch 83/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2896 - val_loss: 0.2752\n",
      "Epoch 84/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.3162 - val_loss: 0.2752\n",
      "Epoch 85/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2681 - val_loss: 0.2705\n",
      "Epoch 86/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2400 - val_loss: 0.2679\n",
      "Epoch 87/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2722 - val_loss: 0.2695\n",
      "Epoch 88/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2231 - val_loss: 0.2712\n",
      "Epoch 89/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.3000 - val_loss: 0.2737\n",
      "Epoch 90/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2033 - val_loss: 0.2648\n",
      "Epoch 91/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2506 - val_loss: 0.2740\n",
      "Epoch 92/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2347 - val_loss: 0.2710\n",
      "Epoch 93/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2262 - val_loss: 0.2652\n",
      "Epoch 94/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2402 - val_loss: 0.2674\n",
      "Epoch 95/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2241 - val_loss: 0.2690\n",
      "Epoch 96/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2561 - val_loss: 0.2684\n",
      "Epoch 97/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2576 - val_loss: 0.2679\n",
      "Epoch 98/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.2457 - val_loss: 0.2648\n",
      "Epoch 99/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.2527 - val_loss: 0.2690\n",
      "Epoch 100/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.2355 - val_loss: 0.2670\n",
      "Epoch 101/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2308 - val_loss: 0.2657\n",
      "Epoch 102/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2411 - val_loss: 0.2675\n",
      "Epoch 103/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2466 - val_loss: 0.2713\n",
      "Epoch 104/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2209 - val_loss: 0.2646\n",
      "Epoch 105/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2871 - val_loss: 0.2657\n",
      "Epoch 106/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2445 - val_loss: 0.2705\n",
      "Epoch 107/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2431 - val_loss: 0.2657\n",
      "Epoch 108/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2631 - val_loss: 0.2671\n",
      "Epoch 109/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.2724 - val_loss: 0.2645\n",
      "Epoch 110/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2384 - val_loss: 0.2648\n",
      "Epoch 111/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2438 - val_loss: 0.2695\n",
      "Epoch 112/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2364 - val_loss: 0.2667\n",
      "Epoch 113/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2019 - val_loss: 0.2642\n",
      "Epoch 114/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2943 - val_loss: 0.2676\n",
      "Epoch 115/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.2790 - val_loss: 0.2643\n",
      "Epoch 116/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2472 - val_loss: 0.2668\n",
      "Epoch 117/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2454 - val_loss: 0.2705\n",
      "Epoch 118/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2403 - val_loss: 0.2680\n",
      "Epoch 119/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2008 - val_loss: 0.2680\n",
      "Epoch 120/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2520 - val_loss: 0.2711\n",
      "Epoch 121/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2678 - val_loss: 0.2579\n",
      "Epoch 122/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2259 - val_loss: 0.2701\n",
      "Epoch 123/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2840 - val_loss: 0.2714\n",
      "Epoch 124/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2284 - val_loss: 0.2602\n",
      "Epoch 125/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2190 - val_loss: 0.2666\n",
      "Epoch 126/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.2215 - val_loss: 0.2708\n",
      "Epoch 127/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2514 - val_loss: 0.2700\n",
      "Epoch 128/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.1859 - val_loss: 0.2711\n",
      "Epoch 129/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2136 - val_loss: 0.2677\n",
      "Epoch 130/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2307 - val_loss: 0.2696\n",
      "Epoch 131/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2360 - val_loss: 0.2691\n",
      "Epoch 132/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.2050 - val_loss: 0.2684\n",
      "Epoch 133/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2262 - val_loss: 0.2702\n",
      "Epoch 134/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2302 - val_loss: 0.2710\n",
      "Epoch 135/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2316 - val_loss: 0.2750\n",
      "Epoch 136/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2304 - val_loss: 0.2698\n",
      "Epoch 137/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.2392 - val_loss: 0.2717\n",
      "Epoch 138/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.1915 - val_loss: 0.2718\n",
      "Epoch 139/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2174 - val_loss: 0.2772\n",
      "Epoch 140/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.1926 - val_loss: 0.2735\n",
      "Epoch 141/150\n",
      "\u001B[1m11/11\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.2349 - val_loss: 0.2774\n",
      "Epoch 141: early stopping\n",
      "Restoring model weights from the end of the best epoch: 121.\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023B3BE11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m1/2\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023B3BE11620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "Keras FUNC training stopped after 90 epochs.\n",
      "\n",
      "Model: MLP_func, mse : 50569612.93799884, r2 : -0.02184793695282461, mae : 2637.2220096214146, rmse 7111.231464240131\n",
      "Keras TECH training stopped after 150 epochs.\n",
      "\n",
      "Model: MLP_tech, mse : 35802917.996613, r2 : 0.2765390959866437, mae : 1984.4902685576794, rmse 5983.553960366114\n",
      "Keras COMB training stopped after 150 epochs.\n",
      "\n",
      "Model: MLP_comb, mse : 28789450.71286396, r2 : 0.418258532984745, mae : 1599.851739285039, rmse 5365.580184179896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28789450.71286396, 0.418258532984745, 1599.851739285039)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f489e8b056edac30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
