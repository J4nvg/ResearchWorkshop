Best DTR tech: {'max_depth': 8, 'min_samples_leaf': 2, 'min_samples_split': 2}
Best DTR func: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2}
Best XGB tech: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}
Best XGB func: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}

=== Final test performance (after grid search) ===

Model: DTR_tech (GS), mse : 34438238.651713766, r2 : 0.30411484142288325, mae : 1598.304027585109, rmse 5868.410232057211

Model: DTR_func (GS), mse : 42696594.522427104, r2 : 0.1372402418593649, mae : 2347.112982465331, rmse 6534.263120079196

Model: XGB_tech (GS), mse : 38770120.982825756, r2 : 0.21658154294586407, mae : 1964.8140569574693, rmse 6226.56574548328

Model: XGB_func (GS), mse : 45341274.26527108, r2 : 0.08379983798600688, mae : 2376.1983079349293, rmse 6733.592968488004

Best MAE (neg): -0.8293701511870563
Best TECH_MODEL hyperparameters: {
'activation': 'relu', 'alpha': 0.0001,
'batch_size': 8, 'early_stopping': True, 'hidden_layer_sizes': (16, 16),
'learning_rate': 'constant',
'learning_rate_init': 0.001,
'solver': 'adam',
'validation_fraction': 0.1}

Model: MLP_tech_sklearn, mse : 34782945.410229415, r2 : 0.2971494353305967, mae : 1874.6461030603741, rmse 5897.706792493962
Fitting 5 folds for each of 1080 candidates, totalling 5400 fits
Best MAE (neg): -1.008209619454355

Best FUNC_MODEL hyperparameters: {
'activation': 'relu', 'alpha': 1e-05,
'batch_size': 32, 'early_stopping': True,
'hidden_layer_sizes': (16, 32), 'learning_rate': 'constant',
'learning_rate_init': 0.01, 'solver': 'adam',
'validation_fraction': 0.1}

Model: MLP_func_sklearn, mse : 48156347.3202528, r2 : 0.026916337668703716, mae : 2578.3016843552723, rmse 6939.477452968113


Best MAE (neg): -0.8293701511870563
Best hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 8, 'early_stopping': True, 'hidden_layer_sizes': (16, 16), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'solver': 'adam', 'validation_fraction': 0.1}

Model: MLP_tech_sklearn, mse : 34782945.410229415, r2 : 0.2971494353305967, mae : 1874.6461030603741, rmse 5897.706792493962
Fitting 5 folds for each of 1080 candidates, totalling 5400 fits
Best MAE (neg): -1.008209619454355
Best hyperparameters: {'activation': 'relu', 'alpha': 1e-05, 'batch_size': 32, 'early_stopping': True, 'hidden_layer_sizes': (16, 32), 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'solver': 'adam', 'validation_fraction': 0.1}

Model: MLP_func_sklearn, mse : 48156347.3202528, r2 : 0.026916337668703716, mae : 2578.3016843552723, rmse 6939.477452968113


OUTLIER REMOVAL\
